{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Damian Garayalde\n",
    "\n",
    "damiangarayalde@gmail.com\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a RAG with ChromaDB and ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we can find first a sample of how a document can be prepared and added into Chroma DB.    \n",
    "Then we create a RAG methon and use a LLM (ChatGPT) to answer questions based on the output of queryng the DB. \n",
    "\n",
    "First we need to load the file info and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'parsed_text.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "filtered_text = ' '.join(lines)\n",
    "\n",
    "pdf_texts= [filtered_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 6194\n"
     ]
    }
   ],
   "source": [
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damiangarayalde/Desktop/Work - Github Repos/AI/Advanced Retrieval for AI with Chroma/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 6194\n"
     ]
    }
   ],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "   \n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6194"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "\n",
    "chroma_collection = chroma_client.create_collection(\"historia.txt\", embedding_function=embedding_function)\n",
    "\n",
    "ids = [str(i) for i in range(len(token_split_texts))]\n",
    "\n",
    "# The .add method will embedd the token_split_texts using the embedding_function specified above\n",
    "\n",
    "chroma_collection.add(ids=ids, documents=token_split_texts)\n",
    "\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ChatGPT API connection\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The information provided in the 'content' is the key for how the system will behave. \n",
    "# Feel free to modify it and test different scenarios\n",
    "\n",
    "def rag(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Eres un asistente de aprendizaje. Tus usuarios son estudiantes que hacen preguntas sobre información contenida en un texto de historia. Se te mostrará la pregunta del usuario y la información relevante del texto. Responde la pregunta del usuario utilizando solo esta información.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Pregunta: {query}. \\n Información: {information}\"}\n",
    "    ]\n",
    "    \n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def rag(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
    "#     information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"You are a learning assistant. Your users are students asking questions about information contained in a Ted talk transcript.\"\n",
    "#             \"You will be shown the user's question, and the relevant information from the lecture transcript. Answer the user's question using only this information.\"\n",
    "#         },\n",
    "#         {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "#     ]\n",
    "    \n",
    "#     response = openai_client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         messages=messages,\n",
    "#     )\n",
    "#     content = response.choices[0].message.content\n",
    "#     return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tedtalk_answer(question, detail_retrieved_docs = True ):\n",
    "\n",
    "    results = chroma_collection.query(query_texts=[question], n_results=5)\n",
    "\n",
    "    # Under the hood the .query() method will embedd the query using the same embedding funtion used when adding the documents. \n",
    "    # Here is where chroma_db searchs for the documents that look similar to the query and then return some documents (5 here)\n",
    "\n",
    "    retrieved_documents = results['documents'][0]\n",
    "\n",
    "    # If required we can list the retrieved fragments:\n",
    "    if detail_retrieved_docs==True:\n",
    "\n",
    "        print('Los fragmentos que poseen una mayor relacion con la pregunta son: \\n')\n",
    "\n",
    "        for document in retrieved_documents:\n",
    "            print(document)\n",
    "        \n",
    "        print('\\n La respuesta construida en base a dichos fragmentos es: ')\n",
    "\n",
    "\n",
    "    output = rag(query=question, retrieved_documents=retrieved_documents)\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main idea of the lecture is that attention, alertness, sleep, repetition, breaks, and mistakes can be used to improve learning. Paying attention is important for learning, and when we are fully focused on a task, we are more likely to retain information for the long term. Repetition is key in learning, as it is not enough to hear or see something once and expect to remember it forever. It is important to prioritize sleep before studying to improve alertness, and to study after learning to retain information for the long term. The hippocampus, which is important for learning and memory, keeps track of information like a diary, but only for the short term.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test  without detailing the retrieved fragments:\n",
    "\n",
    "get_tedtalk_answer( \"What is main idea of the lecture?\" , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fragments that show a closest match to the question are: \n",
      "\n",
      "37. para explicar el alcance del poder papal debemos remontarnos a los siglos xiv y xv, distinguiendo entonces dos corrientes ideologicas : la cesarista, que postulaba la preeminencia del poder civil sobre el religioso, y la teocratica, que consideraba al papa como senor universal del mundo, como autoridad suprema tanto en el orden temporal como en el espiritual\n",
      "el de la religion y la felicidad de sus subditos ”. desde su misma publicacion se desato una polemica acerca de la autenticidad de este documento papal, sosteniendose que su texto contenia alguna maliciosa interpolacion. sin embargo, los modernos estudios han confirmado su total autenticidad, senalando que leon xii expidio conscientemente el breve, aunque bajo la fuerte presion del embajador'espanol en roma, don antonio vargas laguna.\n",
      "anos siguientes, el enfoque papal fue modificandose no solo por una mejor comprension del caso americano, sino tambien por cierto distanciamiento en las relaciones diplomaticas con espana. ese cambio hizo que la santa sede retomara su proverbial prudencia y atendiera principalmente a las necesidades de las almas, dejando a un lado el problema politico. 663. no obstante, el 24 de setiembre de 1824 el papa leon xii,\n",
      ". de acuerdo con esa posicion, el alcance de la bula papal quedaba reducido a la concesion de un derecho para difundir el evangelio y proteger su predicacion, negandosele valor juridico como donacion temporal.\n",
      "e ) el proveniente de las bulas de la santa cruzada era originariamente un derecho eclesiastico asignado en espana para la lucha contra los musulmanes. ingreso en las arcas reales por concesion especial del papa, y consistia en un monto que variaba de acuerdo con la indulgencia contenida en la bula y la capacidad economica del beneficiado.\n",
      "\n",
      " And the rag output is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Las bulas papales eran documentos emitidos por el papa que otorgaban derechos especiales, como por ejemplo la concesión de un derecho para difundir el evangelio y proteger su predicación. Estos documentos no tenían valor jurídico como donación temporal.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test detailing the retrieved fragments:\n",
    "\n",
    "get_tedtalk_answer( \"cuales son las bulas papales\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
